name: SmartRLM - RLM with Auto Early Exit

on:
  workflow_dispatch:
    inputs:
      tarefa:
        description: 'Task/message from user'
        required: true
        type: string
      
      contexto:
        description: 'Context or user history'
        required: false
        type: string
        default: ''
      
      modelo:
        description: 'Ollama model'
        required: false
        type: choice
        options:
          - qwen3:4b
          - mistral:latest
          - phi3:latest
        default: 'qwen3:4b'
      
      confianca:
        description: 'Confidence threshold (0.0-1.0)'
        required: false
        type: string
        default: '0.90'

jobs:
  smartrlm:
    runs-on: self-hosted
    timeout-minutes: 10

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install ollama

      - name: Create output directory
        run: mkdir -p logs/smartrlm

      - name: Run SmartRLM
        env:
          OLLAMA_HOST: http://ollama:11434
        run: |
          python rlm/smart_rlm.py \
            --tarefa "${{ github.event.inputs.tarefa }}" \
            --contexto "${{ github.event.inputs.contexto }}" \
            --modelo "${{ github.event.inputs.modelo }}" \
            --confianca "${{ github.event.inputs.confianca }}" \
            --verbose > logs/smartrlm/output_${{ github.run_id }}.log 2>&1 || true

      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: smartrlm-result-${{ github.run_id }}
          path: logs/smartrlm/
          retention-days: 7

      - name: Display output
        if: always()
        run: |
          echo "=== SmartRLM Result ==="
          tail -30 logs/smartrlm/output_${{ github.run_id }}.log
